# Semantic_Search
This project is an implementation of a semantic search system using large language models and vector embeddings in Python. The goal of semantic search is to retrieve relevant and meaningful information from a large collection of documents, based on the natural language query of the user. Semantic search is different from traditional keyword-based search, as it considers the meaning and context of the query and the documents, rather than just matching words.
# Vector Storage System
The first step of this project is to develop a personalized vector storage system using Python. This system is responsible for efficiently storing and retrieving vector embeddings for the documents and the queries. Vector embeddings are numerical representations of natural language that capture the semantic and syntactic features of the text. They are generated by large language models, such as BERT, GPT-3, or XLNet, that are trained on massive amounts of text data.
The vector storage system uses a custom data structure that allows fast and scalable access to the vector embeddings. The data structure is based on a hash table that maps each document or query ID to its corresponding vector embedding. The vector embeddings are stored as numpy arrays, which are optimized for mathematical operations and memory usage. 
# Cosine Similarity Function
The second step of this project is to implement a cosine similarity function, utilizing numpy dot product, to retrieve semantic information from the vector store. Cosine similarity is a measure of how similar two vectors are, based on the angle between them. It ranges from -1 to 1, where 1 means the vectors are identical, 0 means they are orthogonal, and -1 means they are opposite.
The cosine similarity function takes two inputs: a query vector and a document vector. It computes the dot product between them, and divides it by the product of their norms. The result is a scalar value that represents how semantically related the query and the document are. The higher the cosine similarity, the more relevant the document is to the query. PDF Text
# Extraction and Sentence Tokenization
The third step of this project is to employ pypdf2 library to extract text and employ sentence tokenizer to generate embeddings for text chunks. Pypdf2 is a Python library that can read and write PDF files. It can be used to extract text from PDF documents, such as books, articles, reports, etc. Sentence tokenizer is a tool that can split text into sentences, based on punctuation and grammar rules.
The PDF text extraction and sentence tokenization steps are necessary to prepare the input data for the semantic search system. They allow the system to process each document as a collection of sentences, rather than as a single block of text. This way, the system can generate more fine-grained and accurate vector embeddings for each sentence, and return more specific and relevant results for the query.
# Conclusion
This project demonstrates how to use large language models and vector embeddings to build a semantic search system in Python. The project shows how to develop a personalized vector storage system using Python to efficiently store vector embeddings, how to implement cosine similarity function using numpy dot product to retrieve semantic information from vector store, how to employ pypdf2 library to extract text and employ sentence tokenizer to generate embeddings for text chunks. The project is a useful and practical example of applying natural language processing and information retrieval techniques to solve real-world problems.

